

# === README.md ===

# sensible-fitting
An overarching framework wrapping fitting libraries/functions providing a common API for different backends. It (opinionatedly) focuses on fit functions related to AMO physics. It helps to guess seed parameters and provides a unified and sensible API that is quick to use. This library is currently just used in our labs, but I indend to make it 'nice' for public good at some point.

This project was born out of the desire to replace the (`oitg`)[https://github.com/OxfordIonTrapGroup/oitg]
dependency of (`ndscan`)[https://github.com/OxfordIonTrapGroup/ndscan]. This dependency primarily comes through the plotting functions.


# === dump_code.py ===

#!/usr/bin/env python3
from pathlib import Path

ROOT = Path(".").resolve()
OUT = Path("repo_dump.txt")

INCLUDE = {".py", ".toml", ".md"}
EXCLUDE_DIRS = {".git", ".venv", "__pycache__", ".pytest_cache", ".mypy_cache", "dist", "build"}
EXCLUDE_FILES_SUFFIX = {".pyc"}

def should_skip(path: Path) -> bool:
    if any(part in EXCLUDE_DIRS for part in path.parts):
        return True
    if path.suffix in EXCLUDE_FILES_SUFFIX:
        return True
    return False

files = []
for p in ROOT.rglob("*"):
    if not p.is_file():
        continue
    if should_skip(p):
        continue
    if p.suffix not in INCLUDE:
        continue
    files.append(p)

files.sort()

with OUT.open("w", encoding="utf-8") as f:
    for p in files:
        rel = p.relative_to(ROOT)
        f.write(f"\n\n# === {rel} ===\n\n")
        try:
            f.write(p.read_text(encoding="utf-8"))
        except UnicodeDecodeError:
            f.write("<binary or non-utf8 file skipped>\n")

print(f"Wrote {OUT} with {len(files)} files.")


# === examples/01_line.py ===

import numpy as np
import matplotlib.pyplot as plt
from sensible_fitting import Model


def line(x, m, b):
    return m * x + b


model = (
    Model.from_function(line, name="straight line")
    .bound(m=(-10, 10), b=(-10, 10))
)

rng = np.random.default_rng(0)
x = np.linspace(0, 10, 50)
y_true = line(x, 2.0, -1.0)
sigma = 0.6
y = y_true + rng.normal(0, sigma, size=x.size)

# Model.fit now always returns a Run
run = model.fit(x=x, y=(y, sigma)).squeeze()
res = run.results

print(res["m"].value, "±", res["m"].stderr)
print(res["b"].value, "±", res["b"].stderr)
print(res.summary(digits=4))

fig, ax = plt.subplots()
ax.errorbar(x, y, yerr=sigma, fmt="o", ms=4, capsize=2, label="data")

xg = np.linspace(x.min(), x.max(), 400)
ax.plot(xg, run.model.eval(xg, params=res.params), label="fit")

band = run.band(xg, nsamples=400, level=2)
ax.fill_between(xg, band.low, band.high, alpha=0.2, label="~2σ band")

ax.legend()
ax.set_xlabel("x")
ax.set_ylabel("y")
plt.show()


# === examples/02_batch_common_x.py ===

import numpy as np
import matplotlib.pyplot as plt
from sensible_fitting import models

model = (
    models.sinusoid(name="wave")
    .fix(offset=0.0, phase=np.pi / 3)
    .bound(amplitude=(0.2, 5.0), frequency=(1.0, 6.0))
    .guess(frequency=2.8)
)

rng = np.random.default_rng(2)
N_SYSTEMS, N = 4, 250
x = np.linspace(0, 1, N)

A0, F0 = 2.0, 3.0
A = A0 * (1 + 0.05 * rng.normal(size=N_SYSTEMS))
F = F0 * (1 + 0.02 * rng.normal(size=N_SYSTEMS))

sigma = 0.2
y_clean = np.stack([model.eval(x, amplitude=A[i], frequency=F[i]) for i in range(N_SYSTEMS)])
y = y_clean + rng.normal(0, sigma, size=y_clean.shape)

run = model.fit(x=x, y=(y, sigma))
res = run.results
print(res.summary(digits=4))

fig, axs = plt.subplots(2, 2, figsize=(10, 7), sharex=True, sharey=True)
axs = axs.ravel()
xg = np.linspace(x.min(), x.max(), 500)

for i, ax in enumerate(axs):
    ax.errorbar(x, y[i], yerr=sigma, fmt=".", ms=3, label=f"data {i}")
    sub = run[i]
    yi = sub.model.eval(xg, params=sub.results.params)
    ax.plot(xg, yi, label="fit")
    band = sub.band(xg, nsamples=300, level=2)
    ax.fill_between(xg, band.low, band.high, alpha=0.2)
    ax.set_title(f"system {i}")
    ax.legend()

plt.show()


# === examples/03_batch_ragged_x.py ===

import numpy as np
from sensible_fitting import models

model = models.straight_line().guess(m=0.0, b=0.0)

rng = np.random.default_rng(123)
xs = []
ys = []

for i in range(3):
    n = 30 + 10 * i
    x = np.sort(rng.uniform(-2, 2, size=n))
    sigma = 0.1 + 0.05 * rng.random(size=n)
    y = 0.5 * x - 0.1 + rng.normal(0, sigma)
    xs.append(x)
    ys.append((y, sigma))

run = model.fit(x=xs, y=ys)
print(run.results.summary(digits=4))


# === examples/04_backend_swap.py ===

import numpy as np
import matplotlib.pyplot as plt
from sensible_fitting import Model


def line(x, m, b):
    return m * x + b


# bounds (useful for future Bayesian backends)
model = Model.from_function(line).bound(m=(-10, 10), b=(-10, 10))

rng = np.random.default_rng(5)
x = np.linspace(0, 4, 50)
sigma = 0.3
y = line(x, 1.7, -0.4) + rng.normal(0, sigma, size=x.size)

run_cf = model.fit(x=x, y=(y, sigma), backend="scipy.curve_fit").squeeze()

fig, ax = plt.subplots()
ax.errorbar(x, y, yerr=sigma, fmt=".", label="data")

xg = np.linspace(x.min(), x.max(), 400)
ax.plot(xg, run_cf.model.eval(xg, params=run_cf.results.params), label="curve_fit")

band_cf = run_cf.band(xg, level=2, method="covariance")
ax.fill_between(xg, band_cf.low, band_cf.high, alpha=0.2, label="curve_fit ~2σ")

ax.legend()
plt.show()


# === examples/05_derived_params.py ===

import numpy as np
from sensible_fitting import Model


def gaussian(x, amp, mu, sigma):
    return amp * np.exp(-0.5 * ((x - mu) / sigma) ** 2)


model = (
    Model.from_function(gaussian)
    .bound(amp=(0, None), sigma=(1e-6, None))
    .guess(mu=0.0, amp=1.0)  # or whatever you like as typical
    .derive("fwhm", lambda p: 2.354820045 * p["sigma"], doc="Full-width at half maximum")
)


rng = np.random.default_rng(0)
x = np.linspace(-3, 3, 200)
sigma_y = 0.05
y = model.eval(x, amp=1.0, mu=0.2, sigma=0.7) + rng.normal(0, sigma_y, size=x.size)

run = model.fit(x=x, y=(y, sigma_y)).squeeze()
res = run.results

print("sigma:", res["sigma"].value)
print("fwhm :", res["fwhm"].value, "(derived:", res["fwhm"].derived, ")")


# === examples/06_fit_on_off.py ===

import numpy as np
import matplotlib.pyplot as plt
from sensible_fitting import models

fit_data = True  # True => fit. False => plot seed only.

model = (
    models.sinusoid(name="wave")
    .fix(offset=0.0, phase=np.pi / 3)
    .bound(amplitude=(0.2, 5.0), frequency=(1.0, 6.0))
    .guess(frequency=2.8)
)

rng = np.random.default_rng(2)
N = 25
x = np.linspace(0, 1, N)

sigma = 0.6
y = model.eval(x, amplitude=2.0, frequency=3.0) + rng.normal(0, sigma, size=x.size)

# Always a Run; skip=True => "seed only" mode
run = model.fit(
    x=x,
    y=(y, sigma),
    skip=not fit_data,
).squeeze()

res = run.results

fig, ax = plt.subplots()
ax.errorbar(x, y, yerr=sigma, fmt=".", ms=3, label="data")

xg = np.linspace(x.min(), x.max(), 500)

style = "-" if fit_data else "--"
label = "fit" if fit_data else "seed fit"
ax.plot(xg, run.model.eval(xg, params=res.params), linestyle=style, label=label)

# Band only if covariance exists (i.e. we actually fitted)
if res.cov is not None:
    band = run.band(xg, level=2, nsamples=400)
    ax.fill_between(xg, band.low, band.high, alpha=0.2, label="~2σ band")

ax.legend()
ax.set_xlabel("x")
ax.set_ylabel("y")
plt.show()

print(res.summary(digits=5))


# === examples/07_results_indexing.py ===

import numpy as np

from sensible_fitting import models


def main() -> None:
    """Showcase the Results / ParamsView indexing API."""

    # Batched sinusoid fit: 4 independent datasets sharing the same x grid
    model = (
        models.sinusoid(name="wave")
        .fix(offset=0.0)
        .bound(amplitude=(0.2, 5.0), frequency=(1.0, 6.0))
        .guess(frequency=3.0)
    )

    rng = np.random.default_rng(0)
    N_SYSTEMS, N = 4, 200
    x = np.linspace(0, 1, N)

    A0, F0 = 2.0, 3.0
    A = A0 * (1 + 0.05 * rng.normal(size=N_SYSTEMS))
    F = F0 * (1 + 0.02 * rng.normal(size=N_SYSTEMS))

    sigma = 0.2
    y_clean = np.stack(
        [model.eval(x, amplitude=A[i], frequency=F[i]) for i in range(N_SYSTEMS)]
    )
    y = y_clean + rng.normal(0, sigma, size=y_clean.shape)

    run = model.fit(x=x, y=(y, sigma))
    res = run.results

    print("batch_shape:", res.batch_shape)
    print()

    # 1) Parameter by name across all batches
    freq_all = res["frequency"].value  # shape (4,)
    print("frequency (all batches):", freq_all)

    # 2) Single batch, parameter by name
    freq_0 = res[0]["frequency"].value  # scalar
    print("frequency[0]:", freq_0)

    # 3) Slice of batches
    freq_01 = res[0:2]["frequency"].value  # shape (2,)
    print("frequency[0:2]:", freq_01)

    print()

    # 4) Parameter by *index* via .params
    # Order follows the model function signature: (amplitude, offset, frequency, phase)
    first_param_all = res.params[0].value  # amplitude, shape (4,)
    print("param[0] (all batches):", first_param_all)

    first_param_0 = res[0].params[0].value  # amplitude, batch 0
    print("param[0] (batch 0):", first_param_0)

    print()

    # 5) Multi-param by name -> MultiParamView
    fp = res["frequency", "phase"]  # MultiParamView
    print("multi names:", fp.names)
    print("multi value shape:", fp.value.shape)  # (4, 2)

    freq_col = fp.value[:, 0]
    phase_col = fp.value[:, 1]
    print("freq_col:", freq_col)
    print("phase_col:", phase_col)

    print()

    # 6) Multi-param by index via .params
    # Here: indices 1 and 2 -> (offset, frequency)
    mp_idx = res.params[1, 2]
    print("mp_idx names:", mp_idx.names)
    print("mp_idx value shape:", mp_idx.value.shape)

    print()

    # 7) Using uncertainties: value ± stderr packaged as a uarray
    freq_u = res["frequency"].u
    print("frequency as uarray:", freq_u)


if __name__ == "__main__":
    main()


# === examples/08_hierarchical_indexing.py ===

"""
Example: fits of fits (hierarchical use).

1) For each realisation r:
   - Generate 5 sinusoids with frequencies f_i = a_true + b_true * i, i=0..4.
   - Batch-fit the 5 sinusoids to extract the frequencies f_i (with errors).
   - Fit a straight line f(i) = a + b*i to those 5 frequencies.
   - Store (a_hat, b_hat).

2) Plot the cloud of (a_hat, b_hat) over several realisations.
"""

import numpy as np
import matplotlib.pyplot as plt

from sensible_fitting import Model, models


def main() -> None:
    rng = np.random.default_rng(123)

    # Sinusoid model for the first-level fits
    sin_model = (
        models.sinusoid(name="wave")
        .fix(offset=0.0, phase=0.0)
        .bound(amplitude=(0.5, 3.0), frequency=(0.5, 6.0))
        .guess(frequency=3.0)
    )

    # Straight line model: f(i) = a + b * i
    def line(i, a, b):
        return a + b * i

    line_model = (
        Model.from_function(line, name="linear frequency")
        .guess(a=2.5, b=0.5)
    )


    N_REAL = 3       # number of realisations of (a_true, b_true)
    N_SYSTEMS = 5    # number of sinusoids per realisation
    N = 300          # points per sinusoid

    x = np.linspace(0.0, 1.0, N)
    sigma_y = 0.2

    a_hats = []
    b_hats = []

    for r in range(N_REAL):
        # True linear relation for this realisation
        a_true = 2.5 + 0.2 * rng.normal()
        b_true = 0.5 + 0.05 * rng.normal()

        idx = np.arange(N_SYSTEMS, dtype=float)
        freq_true = a_true + b_true * idx

        # Generate 5 sinusoids with those frequencies, batched
        y_clean = []
        for i in range(N_SYSTEMS):
            y_clean.append(sin_model.eval(x, amplitude=1.5, frequency=freq_true[i]))
        y_clean = np.stack(y_clean, axis=0)

        y = y_clean + rng.normal(0, sigma_y, size=y_clean.shape)

        # First-level batch fit: get frequency per system
        run_sin = sin_model.fit(x=x, y=(y, sigma_y))
        res_sin = run_sin.results

        freqs = res_sin["frequency"].value
        freq_err = res_sin["frequency"].stderr

        # Second-level fit: line through frequencies vs. index
        run_line = line_model.fit(x=idx, y=(freqs, freq_err)).squeeze()
        res_line = run_line.results

        a_hat = res_line["a"].value
        b_hat = res_line["b"].value

        a_hats.append(a_hat)
        b_hats.append(b_hat)

        print(
            f"realisation {r}: "
            f"true (a,b)=({a_true:.3f}, {b_true:.3f}), "
            f"fit (a,b)=({a_hat:.3f}, {b_hat:.3f})"
        )

    a_hats = np.asarray(a_hats)
    b_hats = np.asarray(b_hats)

    # Scatter of the (a_hat, b_hat) pairs
    fig, ax = plt.subplots()
    ax.errorbar(a_hats, b_hats, fmt="o")
    ax.set_xlabel("a_hat")
    ax.set_ylabel("b_hat")
    ax.set_title("Fits-of-fits: (a, b) from each realisation")
    plt.show()


if __name__ == "__main__":
    main()


# === examples/09_new_feat_example.py ===

import numpy as np
import matplotlib.pyplot as plt

from sensible_fitting import models


# --- Build a model with a pure guesser ---------------------------------------

base_model = models.sinusoid(name="seed-demo")

@base_model.guesser
def tweak_guess(x, y, g):
    if g.is_unset("amplitude"):
        g.amplitude = 1.0
    if g.is_unset("offset"):
        g.offset = 0.0
    if g.is_unset("frequency"):
        g.frequency = 3.0
    if g.is_unset("phase"):
        g.phase = 0.0


# Pure builder: attach the guesser, get a new model
model = base_model.with_guesser(tweak_guess)

# --- Generate synthetic data --------------------------------------------------

rng = np.random.default_rng(0)
N = 150
x = np.linspace(0, 1, N)

true_params = dict(amplitude=2.0, offset=0.1, frequency=3.2, phase=0.3)
sigma = 0.25

y_clean = models.sinusoid_func(x, **true_params)
y = y_clean + rng.normal(0, sigma, size=x.size)

# --- 1) Use Model.seed(...) to look at the seed curve ------------------------

seed_params = model.seed(x=x, y=(y, sigma))
print("Seed parameters:")
for name, pv in seed_params.items():
    print(f"  {name:>10s} = {pv.value:.4g}")

# --- 2) Do a seed-only 'fit' (skip=True) -------------------------------------

run_seed = model.fit(
    x=x,
    y=(y, sigma),
    skip=True,          # seed-only mode
).squeeze()

# run_seed.results.params == run_seed.results.seed here (up to fixed params)

# --- 3) Do a real fit --------------------------------------------------------

run_fit = model.fit(
    x=x,
    y=(y, sigma),
).squeeze()

res = run_fit.results
print("\nFitted parameters:")
print(res.summary(digits=4))

# --- 4) Per-call seed overriding everything ----------------------------------

forced_seed = {"frequency": 6.0}
run_forced_seed = model.fit(
    x=x,
    y=(y, sigma),
    seed=forced_seed,
    skip=True,          # use *only* this seed, no optimisation
).squeeze()

print("\nForced-seed parameters (seed={'frequency': 6.0}, skip=True):")
for name, pv in run_forced_seed.results.seed.items():
    print(f"  {name:>10s} = {pv.value:.4g}")

# --- 5) Use Run.predict(...) for fit vs seed curves --------------------------

xg = np.linspace(x.min(), x.max(), 400)

y_seed = run_seed.predict(xg, which="seed")
y_fit = run_fit.predict(xg, which="fit")
y_forced = run_forced_seed.predict(xg, which="seed")  # seed-only run

# --- Plot everything ----------------------------------------------------------

fig, ax = plt.subplots(figsize=(8, 5))

# data
ax.errorbar(x, y, yerr=sigma, fmt="o", ms=3, capsize=2, label="data")

# true underlying curve (for illustration)
ax.plot(xg, models.sinusoid_func(xg, **true_params), "k:", label="true")

# seed-only curve (from seed engine + guesser)
ax.plot(xg, y_seed, "--", label="seed curve")

# fitted curve
ax.plot(xg, y_fit, "-", label="fit curve")

# forced seed curve (frequency fixed to 6.0)
ax.plot(xg, y_forced, "-.", label="seed (frequency=6.0)")

ax.set_xlabel("x")
ax.set_ylabel("y")
ax.legend()
ax.set_title("Seed vs fit vs forced-seed using sensible_fitting")

plt.show()


# === pyproject.toml ===

[project]
name = "sensible-fitting"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
authors = [
    { name = "tomhepz", email = "9285131+tomhepz@users.noreply.github.com" }
]
requires-python = ">=3.14"
dependencies = [
    "lmfit>=1.3.4",
    "numpy>=2.3.5",
    "scipy>=1.16.3",
    "ultranest>=4.4.0",
    "uncertainties>=3.2.3",
]

[build-system]
requires = ["uv_build>=0.9.11,<0.10.0"]
build-backend = "uv_build"

[dependency-groups]
dev = [
    "matplotlib>=3.10.7",
    "pyqt6>=6.10.1",
]


# === src/sensible_fitting/__init__.py ===

"""sensible_fitting public API."""
from .model import Model
from .run import Run, Results, Band
from . import models

__all__ = ["Model", "Run", "Results", "Band", "models"]


# === src/sensible_fitting/backends/__init__.py ===

"""Backend implementations."""


# === src/sensible_fitting/backends/scipy_curve_fit.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, Optional, Tuple

import numpy as np
from scipy.optimize import curve_fit


@dataclass(frozen=True)
class CurveFitResult:
    popt: np.ndarray
    pcov: Optional[np.ndarray]
    success: bool
    message: str = ""


def fit_curve_fit(
    f_wrapped: Callable[..., Any],
    x: Any,
    y: np.ndarray,
    *,
    sigma: Optional[np.ndarray],
    p0: np.ndarray,
    bounds: Tuple[np.ndarray, np.ndarray],
    maxfev: Optional[int] = None,
) -> CurveFitResult:
    kwargs: Dict[str, Any] = {}
    if maxfev is not None:
        kwargs["maxfev"] = int(maxfev)

    try:
        popt, pcov = curve_fit(
            f_wrapped,
            x,
            y,
            p0=p0,
            sigma=sigma,
            # v1: provided errors are absolute measurement errors
            absolute_sigma=(sigma is not None),
            bounds=bounds,
            **kwargs,
        )
        return CurveFitResult(popt=np.asarray(popt, float), pcov=pcov, success=True, message="ok")
    except Exception as e:
        return CurveFitResult(popt=np.asarray(p0, float), pcov=None, success=False, message=str(e))


# === src/sensible_fitting/model.py ===

from __future__ import annotations

from dataclasses import dataclass, replace
from typing import Any, Callable, Dict, List, Literal, Mapping, Optional, Sequence, Tuple, Union

import numpy as np

from .backends.scipy_curve_fit import fit_curve_fit
from .params import DerivedSpec, GuessState, ParameterSpec, ParamView, ParamsView
from .run import Results, Run
from .util import flatten_batch, infer_param_names, is_ragged_batch, prod, unflatten_batch

Guesser = Callable[[Any, Any, GuessState], None]


SeedEngine = Callable[
    ["Model", Any, np.ndarray, Sequence[str], Optional[Mapping[str, float]], np.random.Generator],
    Dict[str, float],
]


@dataclass
class Model:
    """A model wraps a callable and parameter metadata."""
    name: str
    func: Callable[..., Any]
    param_names: Tuple[str, ...]
    params: Tuple[ParameterSpec, ...]
    guessers: Tuple[Guesser, ...] = ()
    derived: Tuple[DerivedSpec, ...] = ()
    seed_engine: Optional[SeedEngine] = None
    meta: Dict[str, Any] = None

    # ---- constructor ----
    @staticmethod
    def from_function(func: Callable[..., Any], *, name: Optional[str] = None) -> "Model":
        names = infer_param_names(func)
        specs = tuple(ParameterSpec(name=n) for n in names)
        return Model(name=name or getattr(func, "__name__", "model"), func=func, param_names=names, params=specs)

    # ---- evaluation ----
    def eval(self, x: Any, *, params: Optional[Mapping[str, Any]] = None, **kwargs) -> Any:
        values: Dict[str, Any] = {}

        if params is not None:
            for k, v in params.items():
                if isinstance(v, ParamView):
                    values[k] = v.value
                else:
                    # If someone passes a dict-like with ["value"], allow it.
                    try:
                        if hasattr(v, "__getitem__"):
                            values[k] = v["value"]  # type: ignore[index]
                        else:
                            values[k] = v
                    except Exception:
                        values[k] = v

        values.update(kwargs)

        # Fill fixed values if absent
        for spec in self.params:
            if spec.fixed and spec.name not in values:
                values[spec.name] = spec.fixed_value

        missing = [n for n in self.param_names if n not in values]
        if missing:
            raise TypeError(f"Missing parameter values for: {missing}")

        args = [x] + [values[n] for n in self.param_names]
        return self.func(*args)

    # ---- builders (pure; return new model) ----
    def fix(self, **fixed: float) -> "Model":
        m = {p.name: p for p in self.params}
        for k, v in fixed.items():
            if k not in m:
                raise KeyError(k)
            m[k] = replace(m[k], fixed=True, fixed_value=float(v))
        return replace(self, params=tuple(m[n] for n in self.param_names))

    def bound(self, **bounds: Tuple[Optional[float], Optional[float]]) -> "Model":
        m = {p.name: p for p in self.params}
        for k, b in bounds.items():
            if k not in m:
                raise KeyError(k)
            lo, hi = b
            m[k] = replace(m[k], bounds=(lo, hi))
        return replace(self, params=tuple(m[n] for n in self.param_names))

    def guess(self, **guesses: float) -> "Model":
        m = {p.name: p for p in self.params}
        for k, g in guesses.items():
            if k not in m:
                raise KeyError(k)
            m[k] = replace(m[k], guess=float(g))
        return replace(self, params=tuple(m[n] for n in self.param_names))

    def prior(self, **priors: Tuple[str, Any]) -> "Model":
        m = {p.name: p for p in self.params}
        for k, p in priors.items():
            if k not in m:
                raise KeyError(k)
            if not isinstance(p, tuple) or len(p) < 1:
                raise TypeError("prior must be like ('normal', 0, 1) etc.")
            kind = str(p[0])
            args = tuple(p[1:])
            m[k] = replace(m[k], prior=(kind, args))
        return replace(self, params=tuple(m[n] for n in self.param_names))

    def derive(self, name: str, func: Callable[[Mapping[str, float]], float], *, doc: str = "") -> "Model":
        if name in self.param_names:
            raise ValueError(f"Derived name {name!r} conflicts with an existing parameter.")
        return replace(self, derived=self.derived + (DerivedSpec(name=name, func=func, doc=doc),))

    def with_guesser(self, fn: Guesser) -> "Model":
        """Return a new Model with `fn` appended to the guesser list."""
        return replace(self, guessers=self.guessers + (fn,))

    def with_seed_engine(self, fn: SeedEngine) -> "Model":
        """Return a new Model with a custom seed engine.

        The engine is called as:
            fn(model, x, y, free_names, user_seed, rng) -> dict(name -> value)
        """
        return replace(self, seed_engine=fn)

    def guesser(self, fn: Optional[Guesser] = None):
        """Decorator helper for defining guesser functions without mutating the model.

        Usage
        -----
        @model.guesser
        def init_gaussian(x, y, g):
            ...

        model = model.with_guesser(init_gaussian)
        """
        def decorator(f: Guesser) -> Guesser:
            return f

        if fn is None:
            return decorator
        return decorator(fn)

    def seed(
        self,
        *,
        x: Any,
        y: Any,
        seed: Optional[Mapping[str, float]] = None,
        data_format: Optional[str] = None,
        parallel: Optional[Literal[None, "auto"]] = None,
        backend_options: Optional[Dict[str, Any]] = None,
        rng: Optional[np.random.Generator] = None,
    ) -> ParamsView:
        """Compute parameter seeds without running the optimiser.

        This is equivalent to a `fit(..., skip=True)` call, but returns the
        seed parameter view directly.
        """
        run = self.fit(
            x=x,
            y=y,
            backend="scipy.curve_fit",
            data_format=data_format,
            parallel=parallel,
            seed=seed,
            skip=True,
            backend_options=backend_options,
            rng=rng,
        )
        res = run.results
        if res.seed is not None:
            return res.seed
        return res.params

    # ---- fitting ----
    def fit(
        self,
        *,
        x: Any,
        y: Any,
        backend: Literal["scipy.curve_fit", "scipy.minimize", "ultranest"] = "scipy.curve_fit",
        data_format: Optional[str] = None,
        parallel: Optional[Literal[None, "auto"]] = None,
        seed: Optional[Mapping[str, float]] = None,
        skip: bool = False,
        backend_options: Optional[Dict[str, Any]] = None,
        rng: Optional[np.random.Generator] = None,
    ) -> Run:
        if rng is None:
            rng = np.random.default_rng()
        backend_options = backend_options or {}

        # v1: only Gaussian inference (data_format None or 'normal')
        if data_format not in (None, "normal"):
            raise NotImplementedError("v1 MVP supports only Gaussian default data inference.")

        datasets: List[Dict[str, Any]] = []
        batch_shape: Tuple[int, ...] = ()

        if is_ragged_batch(x, y):
            for xi, yi in zip(x, y):
                yobs, sigma = _infer_gaussian_payload(yi)
                datasets.append({"x": xi, "y": yobs, "sigma": sigma})
            batch_shape = (len(datasets),)
        else:
            yobs, sigma = _infer_gaussian_payload(y)
            yobs = np.asarray(yobs)
            if yobs.ndim == 1:
                datasets.append({"x": x, "y": yobs, "sigma": sigma})
                batch_shape = ()
            else:
                yflat, batch_shape = flatten_batch(yobs)
                # broadcast sigma if needed
                if sigma is None:
                    sflat = [None] * yflat.shape[0]
                else:
                    sarr = np.asarray(sigma)
                    if sarr.shape == ():
                        sflat = [float(sarr)] * yflat.shape[0]
                    else:
                        sb = np.broadcast_to(sarr, yobs.shape)
                        sb_flat, _ = flatten_batch(sb)
                        sflat = [sb_flat[i] for i in range(sb_flat.shape[0])]
                for i in range(yflat.shape[0]):
                    datasets.append({"x": x, "y": yflat[i], "sigma": sflat[i]})

        free_names, fixed_map = _free_and_fixed(self.params)

        # allocate storage (flattened batch)
        B = len(datasets)
        values = {n: np.empty((B,), dtype=float) for n in self.param_names}
        errors = {n: np.full((B,), np.nan, dtype=float) for n in self.param_names}
        seed_values = {n: np.empty((B,), dtype=float) for n in self.param_names}
        covs: List[Optional[np.ndarray]] = []

        meta: Dict[str, Any] = {
            "mode": "seed" if skip else "fit",
            "free_param_names": list(free_names),
            "success": [],
            "message": [],
        }

        for i, ds in enumerate(datasets):
            xi = ds["x"]
            yi = np.asarray(ds["y"])
            si = ds["sigma"]
            si_arr = None if si is None else np.asarray(si, dtype=float)

            p0_map = _compute_seed_map(self, xi, yi, free_names, rng=rng, user_seed=seed)
            p0 = np.array([float(p0_map[n]) for n in free_names], dtype=float)
            bounds = _bounds_for_free(self.params, free_names)

            # Record the seed params actually used for this dataset
            for j, n in enumerate(free_names):
                seed_values[n][i] = p0[j]
            for n, fv in fixed_map.items():
                seed_values[n][i] = float(fv)

            if skip:
                meta["success"].append(True)
                meta["message"].append("skipped (seed only)")
                for j, n in enumerate(free_names):
                    values[n][i] = p0[j]
                for n, fv in fixed_map.items():
                    values[n][i] = float(fv)
                covs.append(None)
                continue

            if backend != "scipy.curve_fit":
                raise NotImplementedError("v1 MVP implements only backend='scipy.curve_fit'.")

            f_wrapped = _wrap_free_params(self, fixed_map, free_names)
            r = fit_curve_fit(
                f_wrapped,
                xi,
                yi,
                sigma=si_arr,
                p0=p0,
                bounds=bounds,
                maxfev=backend_options.get("maxfev"),
            )
            meta["success"].append(r.success)
            meta["message"].append(r.message)

            # store free values
            for j, n in enumerate(free_names):
                values[n][i] = r.popt[j]
            # fixed values
            for n, fv in fixed_map.items():
                values[n][i] = float(fv)

            if r.pcov is not None:
                pcov = np.asarray(r.pcov, dtype=float)
                covs.append(pcov)
                perr = np.sqrt(np.clip(np.diag(pcov), 0.0, np.inf))
                for j, n in enumerate(free_names):
                    errors[n][i] = perr[j]
            else:
                covs.append(None)

        have_any_cov = any(c is not None for c in covs)

        # Build param views + cov
        if batch_shape == ():
            items: Dict[str, ParamView] = {}
            seed_items: Dict[str, ParamView] = {}
            for n in self.param_names:
                spec = _spec_by_name(self.params, n)
                v = float(values[n][0])
                e = float(errors[n][0])
                sv = float(seed_values[n][0])
                items[n] = ParamView(
                    name=n,
                    value=v,
                    stderr=None if (spec.fixed or not have_any_cov) else e,
                    fixed=spec.fixed,
                    bounds=spec.bounds,
                    derived=False,
                )
                seed_items[n] = ParamView(
                    name=n,
                    value=sv,
                    stderr=None,
                    fixed=spec.fixed,
                    bounds=spec.bounds,
                    derived=False,
                )
            cov = covs[0] if covs and covs[0] is not None else None
            results = Results(
                batch_shape=(),
                params=ParamsView(items),
                seed=ParamsView(seed_items),
                cov=cov,
                backend=backend,
                meta=meta,
            )
        else:
            items = {}
            seed_items = {}
            for n in self.param_names:
                spec = _spec_by_name(self.params, n)
                v = unflatten_batch(values[n], batch_shape)
                e = unflatten_batch(errors[n], batch_shape)
                sv = unflatten_batch(seed_values[n], batch_shape)
                items[n] = ParamView(
                    name=n,
                    value=v,
                    stderr=None if (spec.fixed or not have_any_cov) else e,
                    fixed=spec.fixed,
                    bounds=spec.bounds,
                    derived=False,
                )
                seed_items[n] = ParamView(
                    name=n,
                    value=sv,
                    stderr=None,
                    fixed=spec.fixed,
                    bounds=spec.bounds,
                    derived=False,
                )

            if all(c is not None for c in covs):
                cov = np.stack([c for c in covs], axis=0)
                cov = unflatten_batch(cov, batch_shape)
            else:
                cov = None
            results = Results(
                batch_shape=batch_shape,
                params=ParamsView(items),
                seed=ParamsView(seed_items),
                cov=cov,
                backend=backend,
                meta=meta,
            )

        # Post-fit derived params (v1): depend only on fitted params
        if self.derived:
            if results.batch_shape == ():
                base = {n: float(results.params[n].value) for n in self.param_names}
                extra = {}
                for d in self.derived:
                    dv = float(d.func(base))
                    extra[d.name] = ParamView(name=d.name, value=dv, stderr=None, fixed=True, derived=True)
                results = replace(results, params=ParamsView({**dict(results.params.items()), **extra}))
            else:
                # flatten again
                batch_size = prod(results.batch_shape)
                flat_vals = {
                    n: np.asarray(results.params[n].value).reshape((batch_size,)) for n in self.param_names
                }
                extra_items = {}
                for d in self.derived:
                    out = np.empty((batch_size,), dtype=float)
                    for i in range(batch_size):
                        base = {n: float(flat_vals[n][i]) for n in self.param_names}
                        out[i] = float(d.func(base))
                    extra_items[d.name] = ParamView(
                        name=d.name,
                        value=unflatten_batch(out, results.batch_shape),
                        stderr=None,
                        fixed=True,
                        derived=True,
                    )
                results = replace(results, params=ParamsView({**dict(results.params.items()), **extra_items}))

        run = Run(
            model=self,
            results=results,
            backend=backend,
            data_format=(data_format or "normal"),
            meta=meta,
            data={"x": x, "y": y},
        )

        return run


def _spec_by_name(params: Tuple[ParameterSpec, ...], name: str) -> ParameterSpec:
    for p in params:
        if p.name == name:
            return p
    raise KeyError(name)


def _free_and_fixed(params: Tuple[ParameterSpec, ...]) -> Tuple[List[str], Dict[str, float]]:
    free: List[str] = []
    fixed: Dict[str, float] = {}
    for p in params:
        if p.fixed:
            if p.fixed_value is None:
                raise ValueError(f"Parameter {p.name} is fixed but has no fixed_value.")
            fixed[p.name] = float(p.fixed_value)
        else:
            free.append(p.name)
    return free, fixed


def _bounds_for_free(params: Tuple[ParameterSpec, ...], free_names: List[str]) -> Tuple[np.ndarray, np.ndarray]:
    pmap = {p.name: p for p in params}
    lo: List[float] = []
    hi: List[float] = []
    for n in free_names:
        b = pmap[n].bounds
        if b is None:
            lo.append(-np.inf)
            hi.append(np.inf)
        else:
            lo.append(-np.inf if b[0] is None else float(b[0]))
            hi.append(np.inf if b[1] is None else float(b[1]))
    return (np.array(lo, dtype=float), np.array(hi, dtype=float))


def _wrap_free_params(model: Model, fixed_map: Dict[str, float], free_names: List[str]):
    def f(x, *theta_free):
        kwargs = dict(fixed_map)
        for j, n in enumerate(free_names):
            kwargs[n] = theta_free[j]
        return model.eval(x, **kwargs)

    return f


def _infer_gaussian_payload(y: Any):
    # v1 default inference:
    # y -> unweighted
    # (y, yerr) -> symmetric absolute errors
    # (y, yerr_low, yerr_high) -> asymmetric; approximate to mean sigma for curve_fit
    if isinstance(y, (tuple, list)) and len(y) == 2:
        yobs, yerr = y
        return np.asarray(yobs), yerr
    if isinstance(y, (tuple, list)) and len(y) == 3:
        yobs, ylo, yhi = y
        sigma = 0.5 * (np.asarray(ylo) + np.asarray(yhi))
        return np.asarray(yobs), sigma
    return np.asarray(y), None


def _default_seed_engine(
    model: Model,
    x: Any,
    y: np.ndarray,
    free_names: Sequence[str],
    rng: np.random.Generator,
) -> Dict[str, float]:
    """Built-in seeding strategy.

    Order (before per-call seed overlay):
    1) model-level .guess(...)
    2) user guessers
    """
    pmap = {p.name: p for p in model.params}
    seeds: Dict[str, float] = {}

    # explicit model guesses
    for n in free_names:
        spec = pmap[n]
        if spec.guess is not None:
            seeds[n] = float(spec.guess)

    # user guessers
    if model.guessers:
        gs = GuessState()
        for fn in model.guessers:
            fn(x, y, gs)
        for n, v in gs.to_dict().items():
            if n in free_names and n not in seeds:
                seeds[n] = float(v)

    return seeds


def _compute_seed_map(
    model: Model,
    x: Any,
    y: np.ndarray,
    free_names: List[str],
    rng: np.random.Generator,
    user_seed: Optional[Mapping[str, float]] = None,
) -> Dict[str, float]:
    """Compute initial seeds for the given dataset.

    Precedence per free parameter:

      1) per-call `user_seed` (fit(..., seed=...))
      2) ParameterSpec.guess via model.guess(...)
      3) model guessers (functions registered with with_guesser)
      4) midpoint of finite bounds (with a warning)
      5) else: raise ValueError
    """
    from warnings import warn

    free_names = list(free_names)
    pmap = {p.name: p for p in model.params}

    # 1+2+3 via seed engine (or default)
    if model.seed_engine is not None:
        # custom engine can take user_seed + rng if it wants
        seeds = dict(model.seed_engine(model, x, y, free_names, user_seed, rng))
    else:
        seeds = _default_seed_engine(model, x, y, free_names, rng)

    # keep only known free names
    seeds = {n: float(v) for n, v in seeds.items() if n in free_names}

    # 1) overlay per-call seed (highest precedence)
    if user_seed is not None:
        for n, v in user_seed.items():
            if n in free_names:
                seeds[n] = float(v)

    # 4) fill missing from bounds midpoint if possible
    filled_from_bounds: List[str] = []
    for n in free_names:
        if n in seeds:
            continue
        spec = pmap[n]
        b = spec.bounds
        if b is not None:
            lo, hi = b
            if (
                lo is not None
                and hi is not None
                and np.isfinite(lo)
                and np.isfinite(hi)
            ):
                seeds[n] = float(0.5 * (float(lo) + float(hi)))
                filled_from_bounds.append(n)

    if filled_from_bounds:
        warn(
            "Using mid-point of bounds as seed for parameters: "
            + ", ".join(filled_from_bounds),
            UserWarning,
        )

    # 5) final check
    missing = [n for n in free_names if n not in seeds]
    if missing:
        raise ValueError(
            "Could not determine initial seeds for parameters: "
            + ", ".join(missing)
            + ". Provide seed=..., model.guess(...), a guesser, or finite bounds."
        )

    return seeds



# === src/sensible_fitting/models.py ===

from __future__ import annotations

import numpy as np

from .model import Model


def straight_line_func(x, m, b):
    """Module-level straight line function y = m*x + b."""
    return m * x + b


def straight_line(*, name: str = "straight line") -> Model:
    """Return a straight line Model."""
    return Model.from_function(straight_line_func, name=name)


def sinusoid_func(x, amplitude, offset, frequency, phase):
    """Module-level sinusoid: offset + amplitude * sin(2π f x + phase)."""
    return offset + amplitude * np.sin(2 * np.pi * frequency * x + phase)


def sinusoid(*, name: str = "sinusoid") -> Model:
    """Return a sinusoid Model with sensible default seeding."""
    return Model.from_function(sinusoid_func, name=name)


# === src/sensible_fitting/params.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Mapping, Optional, Sequence, Tuple

import numpy as np

try:
    from uncertainties import unumpy as unp
except Exception:  # pragma: no cover - optional at import time
    unp = None


__all__ = ["ParameterSpec", "DerivedSpec", "ParamView", "ParamsView", "MultiParamView", "GuessState"]


@dataclass(frozen=True)
class ParameterSpec:
    name: str
    fixed: bool = False
    fixed_value: Optional[float] = None
    bounds: Optional[Tuple[Optional[float], Optional[float]]] = None
    guess: Optional[float] = None
    # Stored for Bayesian backends (ignored by curve_fit in v1)
    prior: Optional[Tuple[str, Tuple[Any, ...]]] = None
    meta: Dict[str, Any] = None


@dataclass(frozen=True)
class DerivedSpec:
    """Post-fit derived parameter (v1).

    v1 restriction:
    - computed only AFTER fitting
    - depends only on fitted params, not on other derived params
    """
    name: str
    func: Any  # Callable[[Mapping[str, float]], float]
    doc: str = ""
    meta: Dict[str, Any] = None


@dataclass(frozen=True)
class ParamView:
    """A single parameter view."""
    name: str
    value: Any
    stderr: Any = None
    fixed: Any = False
    bounds: Optional[Tuple[Optional[float], Optional[float]]] = None
    derived: bool = False
    meta: Dict[str, Any] = None

    # Backwards-compatible alias
    @property
    def error(self) -> Any:  # type: ignore[override]
        return self.stderr

    @property
    def u(self):
        """Return an uncertainties uarray/ufloat if stderr is available."""
        if self.stderr is None:
            raise ValueError(f"No stderr available for parameter {self.name!r}.")
        if unp is None:
            raise RuntimeError("uncertainties package is not available.")
        return unp.uarray(self.value, self.stderr)

    def __getitem__(self, key: str) -> Any:
        if key == "value":
            return self.value
        if key in ("error", "stderr"):
            return self.stderr
        if key == "fixed":
            return self.fixed
        if key == "bounds":
            return self.bounds
        if key == "derived":
            return self.derived
        raise KeyError(key)


@dataclass(frozen=True)
class MultiParamView:
    """View over multiple parameters at once.

    value and stderr have shape batch_shape + (len(names),).
    """
    names: Tuple[str, ...]
    value: Any
    stderr: Any = None
    meta: Dict[str, Any] = None

    @property
    def u(self):
        if self.stderr is None:
            raise ValueError("No stderr available for MultiParamView.u.")
        if unp is None:
            raise RuntimeError("uncertainties package is not available.")
        return unp.uarray(self.value, self.stderr)


class ParamsView(Mapping[str, ParamView]):
    """Mapping name -> ParamView, with rich indexing."""

    def __init__(self, items: Mapping[str, ParamView]):
        self._items = dict(items)
        self._names = tuple(self._items.keys())

    def __getitem__(self, key):  # type: ignore[override]
        # Param by name
        if isinstance(key, str):
            return self._items[key]

        # Multi-param by names: ("frequency", "phase") or ["frequency", "phase"]
        if isinstance(key, (tuple, list)) and key and all(isinstance(k, str) for k in key):
            names = tuple(key)
            return self._multi_by_names(names)

        # Param by index
        if isinstance(key, int):
            name = self._names[key]
            return self._items[name]

        # Slice of params -> MultiParamView
        if isinstance(key, slice):
            names = self._names[key]
            return self._multi_by_names(names)

        # Explicit indices -> MultiParamView
        if isinstance(key, (tuple, list)) and key and all(isinstance(k, int) for k in key):
            names = tuple(self._names[i] for i in key)
            return self._multi_by_names(names)

        raise KeyError(key)

    def __iter__(self):
        return iter(self._items)

    def __len__(self):
        return len(self._items)

    def items(self):
        return self._items.items()

    def as_dict(self) -> Dict[str, Any]:
        """Return name->value (extracting .value)."""
        return {k: v.value for k, v in self._items.items()}

    # ---- helpers ----
    def _multi_by_names(self, names: Sequence[str]) -> MultiParamView:
        names = tuple(names)
        if not names:
            raise ValueError("MultiParamView requires at least one parameter name.")

        values = []
        stderrs = []
        have_err = False
        for n in names:
            pv = self._items[n]
            v = np.asarray(pv.value)
            values.append(v)
            if pv.stderr is None:
                stderrs.append(None)
            else:
                stderrs.append(np.asarray(pv.stderr))
                have_err = True

        value_arr = np.stack(values, axis=-1)
        stderr_arr = None
        if have_err:
            filled = []
            for v, e in zip(values, stderrs):
                filled.append(np.zeros_like(v, dtype=float) if e is None else e)
            stderr_arr = np.stack(filled, axis=-1)

        return MultiParamView(names=names, value=value_arr, stderr=stderr_arr)


class GuessState:
    """Mutable guess state passed to guessers.

    Supports:
        g.amplitude = 1.0
        g.is_unset("amplitude")
    """

    def __init__(self):
        object.__setattr__(self, "_d", {})

    def __getattr__(self, name: str) -> Any:
        d = object.__getattribute__(self, "_d")
        if name in d:
            return d[name]
        raise AttributeError(name)

    def __setattr__(self, name: str, value: Any) -> None:
        if name == "_d":
            object.__setattr__(self, name, value)
            return
        d = object.__getattribute__(self, "_d")
        d[name] = value

    def is_unset(self, name: str) -> bool:
        d = object.__getattribute__(self, "_d")
        return name not in d

    def to_dict(self) -> Dict[str, Any]:
        return dict(object.__getattribute__(self, "_d"))


# === src/sensible_fitting/run.py ===

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Literal, Mapping, Optional, Tuple

import numpy as np

from .params import ParamView, ParamsView
from .util import level_to_conf_int, prod, sample_mvn


@dataclass(frozen=True)
class Band:
    low: np.ndarray
    high: np.ndarray
    median: Optional[np.ndarray] = None
    meta: Dict[str, Any] = None


@dataclass(frozen=True)
class Results:
    batch_shape: Tuple[int, ...]
    params: ParamsView
    seed: Optional[ParamsView] = None
    cov: Optional[np.ndarray] = None
    backend: str = ""
    meta: Dict[str, Any] = None

    def __getitem__(self, key):
        # ---- Parameter access sugar ----------------------------------------
        # res["frequency"]           -> ParamView (all batches)
        # res["frequency", "phase"]  -> MultiParamView
        # res[["frequency", "phase"]] same as above
        if isinstance(key, str):
            return self.params[key]

        if isinstance(key, (tuple, list)) and key and all(isinstance(k, str) for k in key):
            return self.params[key]

        # ---- Batch slicing --------------------------------------------------
        idx = key
        if self.batch_shape == ():
            raise IndexError("Scalar Results cannot be indexed; already squeezed.")

        def _slice(v):
            if v is None:
                return None
            a = np.asarray(v)
            if a.shape == ():
                return v
            return a[idx]

        new_items: Dict[str, ParamView] = {}
        for name, pv in self.params.items():
            new_items[name] = ParamView(
                name=name,
                value=_slice(pv.value),
                stderr=_slice(pv.stderr),
                fixed=_slice(pv.fixed) if isinstance(pv.fixed, np.ndarray) else pv.fixed,
                bounds=pv.bounds,
                derived=pv.derived,
                meta=pv.meta,
            )

        new_seed = None
        if self.seed is not None:
            seed_items: Dict[str, ParamView] = {}
            for name, pv in self.seed.items():
                seed_items[name] = ParamView(
                    name=name,
                    value=_slice(pv.value),
                    stderr=_slice(pv.stderr),
                    fixed=_slice(pv.fixed) if isinstance(pv.fixed, np.ndarray) else pv.fixed,
                    bounds=pv.bounds,
                    derived=pv.derived,
                    meta=pv.meta,
                )
            new_seed = ParamsView(seed_items)

        cov = self.cov
        if cov is not None and np.asarray(cov).ndim >= 3:
            cov = np.asarray(cov)[idx]

        new_batch_shape = ()
        for pv in new_items.values():
            a = np.asarray(pv.value)
            if a.shape != ():
                new_batch_shape = a.shape
                break

        return Results(
            batch_shape=tuple(new_batch_shape),
            params=ParamsView(new_items),
            seed=new_seed,
            cov=cov,
            backend=self.backend,
            meta=self.meta,
        )

    def summary(self, digits: int = 4) -> str:
        meta = self.meta or {}
        lines = [f"Results(backend={self.backend!r}, batch_shape={self.batch_shape})"]

        if self.batch_shape == ():
            for name, pv in self.params.items():
                v = pv.value
                e = pv.error
                tag = " (derived)" if pv.derived else ""
                if e is None:
                    lines.append(f"  {name:>12s}: {float(v):.{digits}g}{tag}")
                else:
                    lines.append(f"  {name:>12s}: {float(v):.{digits}g} ± {float(e):.{digits}g}{tag}")
            return "\n".join(lines)

        batch_size = prod(self.batch_shape)
        show = min(batch_size, 10)
        names = list(self.params.keys())

        header = "idx " + " ".join([f"{n:>14s}" for n in names])
        lines.append(header)
        lines.append("-" * len(header))

        for i in range(show):
            row = [f"{i:>3d}"]
            for n in names:
                pv = self.params[n]
                v = np.asarray(pv.value).reshape((batch_size,))[i]
                e = pv.error
                if e is None:
                    row.append(f"{float(v):>14.{digits}g}")
                else:
                    eflat = np.asarray(e).reshape((batch_size,))[i]
                    row.append(f"{float(v):>7.{digits}g}±{float(eflat):<6.{digits}g}")
            lines.append(" ".join(row))

        if batch_size > show:
            lines.append(f"... ({batch_size-show} more)")
        return "\n".join(lines)


@dataclass(frozen=True)
class Run:
    model: Any  # Model
    results: Results
    backend: str
    data_format: str
    meta: Dict[str, Any] = None
    data: Dict[str, Any] = None

    def squeeze(self) -> "Run":
        if self.results.batch_shape == ():
            return self
        batch_size = prod(self.results.batch_shape)
        if batch_size != 1:
            raise ValueError(f"run.squeeze() requires exactly one fit; got batch_size={batch_size}. Slice first.")
        idx = tuple(0 for _ in self.results.batch_shape)
        return self[idx]

    def __getitem__(self, idx) -> "Run":
        sub_results = self.results[idx]

        sub_data = None
        if self.data is not None:
            sub_data = {}
            bs = self.results.batch_shape  # batch shape *before* slicing

            for k, v in self.data.items():
                # Ragged batch stores list-of-datasets; index directly.
                if isinstance(v, list):
                    sub_data[k] = v[idx]

                # Structured payload tuples like (y, sigma) or (y, lo, hi):
                # slice any numpy array that carries the batch dimension.
                elif isinstance(v, tuple):
                    parts = []
                    for part in v:
                        if isinstance(part, np.ndarray) and bs != () and part.shape[: len(bs)] == bs:
                            parts.append(part[idx])
                        else:
                            parts.append(part)
                    sub_data[k] = tuple(parts)

                else:
                    sub_data[k] = v

        return Run(
            model=self.model,
            results=sub_results,
            backend=self.backend,
            data_format=self.data_format,
            meta=self.meta,
            data=sub_data,
        )

    def predict(
        self,
        x: Any,
        *,
        which: Literal["fit", "seed"] = "fit",
        params: Optional[Mapping[str, Any]] = None,
    ) -> Any:
        """Evaluate the model at `x` using fitted or seed parameters.

        which="fit"  -> use results.params
        which="seed" -> use results.seed (if available)
        params=...   -> explicit param mapping; 'which' must be "fit"
        """
        if params is not None and which != "fit":
            raise ValueError("Cannot pass explicit params when which != 'fit'.")

        if params is None:
            if which == "fit":
                p = self.results.params
            elif which == "seed":
                if self.results.seed is None:
                    raise ValueError("No seed parameters available on this Run.")
                p = self.results.seed
            else:
                raise ValueError(f"Unknown value for 'which': {which!r}")
        else:
            p = params

        return self.model.eval(x, params=p)

    def band(
        self,
        x: Any,
        *,
        nsamples: int = 400,
        level: Optional[float] = None,
        conf_int: Optional[Tuple[float, float]] = None,
        method: Literal["auto", "posterior", "covariance"] = "auto",
        rng: Optional[np.random.Generator] = None,
    ) -> Band:
        if self.results.batch_shape != ():
            raise ValueError("run.band() requires a scalar run. Slice first (e.g., run[i].band(...)).")

        if rng is None:
            rng = np.random.default_rng()

        if level is None and conf_int is None:
            level = 2.0
        if level is not None and conf_int is not None:
            raise ValueError("Provide only one of level= or conf_int=.")

        if conf_int is None:
            qlo, qhi = level_to_conf_int(float(level))
        else:
            qlo, qhi = conf_int

        # v1: covariance-only (posterior reserved)
        if method not in ("auto", "covariance"):
            raise NotImplementedError("v1: posterior-based band() is reserved.")

        cov = self.results.cov
        if cov is None:
            raise ValueError("No covariance available for band().")

        meta = self.results.meta or {}
        free_names = meta.get("free_param_names")
        if not free_names:
            raise ValueError("Results.meta['free_param_names'] missing; cannot compute band().")

        mean = np.array([float(self.results.params[n].value) for n in free_names], dtype=float)
        cov = np.asarray(cov, dtype=float)

        theta = sample_mvn(mean, cov, int(nsamples), rng)  # (S,P)

        preds = []
        for s in range(theta.shape[0]):
            p = {name: theta[s, j] for j, name in enumerate(free_names)}
            preds.append(np.asarray(self.model.eval(x, **p)))
        preds = np.stack(preds, axis=0)

        lo = np.quantile(preds, qlo, axis=0)
        hi = np.quantile(preds, qhi, axis=0)
        med = np.quantile(preds, 0.5, axis=0)

        return Band(low=lo, high=hi, median=med, meta={"method": "covariance", "q": (qlo, qhi)})


# === src/sensible_fitting/util.py ===

from __future__ import annotations

import inspect
import math
from typing import Any, Callable, Tuple

import numpy as np


def normal_cdf(z: float) -> float:
    """Standard Normal CDF Φ(z)."""
    return 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))


def level_to_conf_int(level: float) -> Tuple[float, float]:
    """Central interval for a Normal-equivalent ±level sigma."""
    lo = normal_cdf(-float(level))
    hi = normal_cdf(+float(level))
    return (lo, hi)


def infer_param_names(func: Callable[..., Any]) -> Tuple[str, ...]:
    """Infer parameter names from a function signature.

    Conventions:
    - first arg is independent variable container (x)
    - remaining positional/keyword parameters are fit parameters

    v1 restriction:
    - no *args/**kwargs in model functions
    """
    sig = inspect.signature(func)
    params = list(sig.parameters.values())

    if len(params) < 2:
        raise TypeError("Model function must have at least (x, p1, ...).")

    bad_kinds = {inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD}
    for p in params:
        if p.kind in bad_kinds:
            raise TypeError("v1: *args/**kwargs are not supported in model functions.")

    names = [p.name for p in params[1:]]
    if len(set(names)) != len(names):
        raise TypeError("Duplicate parameter names in function signature.")
    return tuple(names)


def prod(shape: Tuple[int, ...]) -> int:
    n = 1
    for s in shape:
        n *= int(s)
    return int(n)


def flatten_batch(arr: np.ndarray) -> Tuple[np.ndarray, Tuple[int, ...]]:
    """Flatten batch dimensions of an array shaped batch_shape + (N,).

    Returns
    -------
    arr_flat : ndarray, shape (B, N) where B=prod(batch_shape)
    batch_shape : tuple
        Empty tuple means scalar/single dataset.
    """
    arr = np.asarray(arr)
    if arr.ndim == 1:
        return arr[None, :], ()
    batch_shape = tuple(arr.shape[:-1])
    B = prod(batch_shape)
    N = arr.shape[-1]
    return arr.reshape(B, N), batch_shape


def unflatten_batch(values: np.ndarray, batch_shape: Tuple[int, ...]) -> np.ndarray:
    """Unflatten arrays with leading dimension B back to batch_shape."""
    values = np.asarray(values)
    if batch_shape == ():
        return values.reshape(())
    return values.reshape(batch_shape + values.shape[1:])


def _jittered_cholesky(cov: np.ndarray, max_tries: int = 6) -> np.ndarray:
    cov = np.asarray(cov, dtype=float)
    cov = 0.5 * (cov + cov.T)
    diag = np.diag(cov)
    scale = float(np.max(diag)) if diag.size else 1.0
    scale = 1.0 if not np.isfinite(scale) or scale <= 0 else scale

    jitter = 0.0
    for i in range(max_tries):
        try:
            return np.linalg.cholesky(cov + jitter * np.eye(cov.shape[0]))
        except np.linalg.LinAlgError:
            jitter = (10.0 ** (-(max_tries - i))) * 1e-6 * scale + (jitter * 10.0 if jitter else 0.0)

    w, v = np.linalg.eigh(cov)
    w = np.clip(w, 0.0, None)
    return v @ np.diag(np.sqrt(w))


def sample_mvn(mean: np.ndarray, cov: np.ndarray, nsamples: int, rng: np.random.Generator) -> np.ndarray:
    """Sample from MVN(mean, cov) robustly. Returns shape (nsamples, P)."""
    mean = np.asarray(mean, dtype=float)
    cov = np.asarray(cov, dtype=float)
    L = _jittered_cholesky(cov)
    z = rng.normal(size=(nsamples, mean.shape[0]))
    return mean[None, :] + z @ L.T


def is_sequence(x: Any) -> bool:
    return isinstance(x, (list, tuple))


def is_ragged_batch(x: Any, y: Any) -> bool:
    """Heuristic: x and y are sequences of equal length => ragged batch."""
    return is_sequence(x) and is_sequence(y) and len(x) == len(y)


def safe_float(x: Any) -> float:
    """Convert numpy scalar / 0-d array to python float."""
    if isinstance(x, np.ndarray) and x.shape == ():
        return float(x.item())
    return float(x)
